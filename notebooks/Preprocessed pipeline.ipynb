{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcBA3ZeLsghx",
        "outputId": "45d145a0-1b1a-40a2-dccf-d0ecada3937f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XnmqJBCcFRo_"
      },
      "outputs": [],
      "source": [
        "# Path to the folder containing your CSV files\n",
        "data_path = '/content/drive/MyDrive/WattVision_Preprocessing/data/raw/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CDhltPn3LRQf"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51-2N0IfLYrk",
        "outputId": "1be52723-6c82-48e6-f8ab-518506209b2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ðŸ”¹ CSV files found: ['smart_6hour_1.csv', 'smart_6hour_2.csv', 'smart_6hour_3.csv', 'smart_6hour_4.csv', 'smart_6hour_5.csv']\n",
            "âœ… Merged shape: (4441793, 32)\n",
            "âœ… Merged file saved to: /content/drive/MyDrive/WattVision_Preprocessing/data/outputs/smart_6hour_merged.csv\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "data_path = '/content/drive/MyDrive/WattVision_Preprocessing/data/raw/'\n",
        "output_dir = '/content/drive/MyDrive/WattVision_Preprocessing/data/outputs/'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# List all CSV files\n",
        "csv_files = [file for file in os.listdir(data_path) if file.endswith('.csv')]\n",
        "print(\"ðŸ”¹ CSV files found:\", csv_files)\n",
        "\n",
        "# Read and merge\n",
        "df_list = [pd.read_csv(os.path.join(data_path, file), low_memory=False) for file in csv_files]\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "print(\"âœ… Merged shape:\", df.shape)\n",
        "\n",
        "merged_path = os.path.join(output_dir, 'smart_6hour_merged.csv')\n",
        "df.to_csv(merged_path, index=False)\n",
        "print(f\"âœ… Merged file saved to: {merged_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# STEP 3: Handle Missing Values\n",
        "# ====================================================================\n",
        "\n",
        "print(\"Handling missing values...\")\n",
        "# Loop through all columns in the DataFrame\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        # Fill missing categorical values with the mode\n",
        "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "    else:\n",
        "        # Fill missing numerical values with the median\n",
        "        df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "print(\"Missing values have been handled.\")\n",
        "print(f\"Total remaining null values: {df.isnull().sum().sum()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTAjL_LouTTi",
        "outputId": "3527d6ea-417c-4426-987c-ee6fbee045f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handling missing values...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2137735863.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n",
            "/tmp/ipython-input-2137735863.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipython-input-2137735863.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].median(), inplace=True)\n",
            "/tmp/ipython-input-2137735863.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna(df[col].mode()[0], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values have been handled.\n",
            "Total remaining null values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUy0-VNjNl_d",
        "outputId": "5adb4755-9d01-4d30-95e9-88b24e79a75b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering complete. Target variable 'NET_IMPORT_kWh' is NOT scaled.\n",
            "\n",
            "--- Descriptive Statistics for UN-SCALED 'NET_IMPORT_kWh' ---\n",
            "count    4.441793e+06\n",
            "mean     1.967881e+03\n",
            "std      1.338486e+04\n",
            "min     -2.176603e+05\n",
            "25%      7.113330e+02\n",
            "50%      1.809573e+03\n",
            "75%      3.143654e+03\n",
            "max      4.603319e+05\n",
            "Name: NET_IMPORT_kWh, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# STEP 4: Feature Engineering\n",
        "# ====================================================================\n",
        "\n",
        "# --- Time-based Features ---\n",
        "# Combine DATE and TIME into a single datetime column before dropping them\n",
        "df['datetime'] = pd.to_datetime(df['DATE'] + ' ' + df['TIME'], errors='coerce')\n",
        "df['year'] = df['datetime'].dt.year\n",
        "df['month'] = df['datetime'].dt.month\n",
        "df['hour'] = df['datetime'].dt.hour\n",
        "df.drop(columns=['DATE', 'TIME', 'datetime'], inplace=True)\n",
        "\n",
        "# --- Create the Target Variable (Un-scaled) ---\n",
        "# Ensure the import/export columns are numeric before subtraction\n",
        "imp = pd.to_numeric(df['TOTAL_IMPORT (kWh)'], errors='coerce').fillna(0)\n",
        "exp = pd.to_numeric(df['TOTAL_EXPORT (kWh)'], errors='coerce').fillna(0)\n",
        "\n",
        "# Calculate NET_IMPORT_kWh in its original kWh unit\n",
        "df['NET_IMPORT_kWh'] = imp - exp\n",
        "\n",
        "# --- Create Additional Ratio Feature ---\n",
        "df['EXPORT_IMPORT_RATIO'] = exp / imp\n",
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "print(\"Feature engineering complete. Target variable 'NET_IMPORT_kWh' is NOT scaled.\")\n",
        "print(\"\\n--- Descriptive Statistics for UN-SCALED 'NET_IMPORT_kWh' ---\")\n",
        "print(df['NET_IMPORT_kWh'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-S8aqFxgNtZz",
        "outputId": "17250fba-48ab-4d6b-8904-b9a0b8cd71ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 792 duplicate rows.\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# STEP 5: Remove Duplicates\n",
        "# ====================================================================\n",
        "\n",
        "initial_rows = df.shape[0]\n",
        "df.drop_duplicates(inplace=True)\n",
        "print(f\"Removed {initial_rows - df.shape[0]} duplicate rows.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gQmzKSc-Nxh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c016bb2-33ff-468f-aec5-b5786dc6dc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing outlier check columns to be numeric...\n",
            "Numeric conversion complete. Starting outlier removal...\n",
            "\n",
            "Outlier removal on 'TOTAL_IMPORT (kWh)':\n",
            "  - Removed 363299 rows.\n",
            "\n",
            "Outlier removal on 'TOTAL_IMPORT - PV1 (kWh)':\n",
            "  - Removed 114330 rows.\n",
            "\n",
            "Outlier removal on 'TR1_TOTAL_IMPORT (kWh)':\n",
            "  - Removed 116499 rows.\n",
            "\n",
            "Outlier removal on 'TR2_TOTAL_IMPORT (kWh)':\n",
            "  - Removed 140617 rows.\n",
            "\n",
            "Outlier removal on 'TR3_TOTAL_IMPORT (kWh)':\n",
            "  - Removed 96962 rows.\n",
            "\n",
            "Outlier removal on 'TR1_TOTAL_IMPORT - PV1 (kWh)':\n",
            "  - Removed 29812 rows.\n",
            "\n",
            "Outlier removal on 'TR2_TOTAL_IMPORT - PV1 (kWh)':\n",
            "  - Removed 31228 rows.\n",
            "\n",
            "Outlier removal on 'TR3_TOTAL_IMPORT - PV1 (kWh)':\n",
            "  - Removed 22740 rows.\n",
            "\n",
            "Outlier removal on 'PHASE_A_CURRENT (A)':\n",
            "  - Removed 172057 rows.\n",
            "\n",
            "Outlier removal on 'PHASE_A_VOLTAGE (V)':\n",
            "  - Removed 89750 rows.\n",
            "\n",
            "Outlier removal on 'FREQUENCY (Hz)':\n",
            "  - Removed 30465 rows.\n",
            "\n",
            "Shape after outlier removal: (3233242, 35)\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# STEP 6: Outlier Removal using IQR\n",
        "# ====================================================================\n",
        "\n",
        "# Define columns to check for outliers\n",
        "columns_to_check_outliers = [\n",
        "    \"TOTAL_IMPORT (kWh)\", \"TOTAL_IMPORT - PV1 (kWh)\",\n",
        "    \"TR1_TOTAL_IMPORT (kWh)\", \"TR2_TOTAL_IMPORT (kWh)\", \"TR3_TOTAL_IMPORT (kWh)\",\n",
        "    \"TR1_TOTAL_IMPORT - PV1 (kWh)\", \"TR2_TOTAL_IMPORT - PV1 (kWh)\", \"TR3_TOTAL_IMPORT - PV1 (kWh)\",\n",
        "    \"PHASE_A_CURRENT (A)\", \"PHASE_A_VOLTAGE (V)\", \"FREQUENCY (Hz)\"\n",
        "]\n",
        "\n",
        "# --- FIX: Convert columns to numeric before outlier removal ---\n",
        "# This ensures that any string values are converted, preventing the TypeError.\n",
        "# 'errors=coerce' will turn any value that can't be converted into NaN (Not a Number).\n",
        "print(\"Forcing outlier check columns to be numeric...\")\n",
        "for col in columns_to_check_outliers:\n",
        "    if col in df.columns:\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# The outlier removal function will now correctly handle any NaN values created.\n",
        "print(\"Numeric conversion complete. Starting outlier removal...\")\n",
        "\n",
        "\n",
        "# Function to remove outliers using the Interquartile Range (IQR) method\n",
        "def remove_outliers_iqr(data, columns):\n",
        "    cleaned_data = data.copy()\n",
        "    for col in columns:\n",
        "        # The quantile function automatically ignores NaN values, so no extra handling is needed here.\n",
        "        Q1 = cleaned_data[col].quantile(0.25)\n",
        "        Q3 = cleaned_data[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        initial_rows = cleaned_data.shape[0]\n",
        "        # Filter the data to keep only the rows within the valid bounds\n",
        "        cleaned_data = cleaned_data[(cleaned_data[col] >= lower_bound) & (cleaned_data[col] <= upper_bound)]\n",
        "        print(f\"\\nOutlier removal on '{col}':\")\n",
        "        print(f\"  - Removed {initial_rows - cleaned_data.shape[0]} rows.\")\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "# Apply outlier removal to the now fully numeric columns\n",
        "df_no_outliers = remove_outliers_iqr(df, columns_to_check_outliers)\n",
        "\n",
        "print(f\"\\nShape after outlier removal: {df_no_outliers.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_kkBT8iN1pV",
        "outputId": "c2fff1c6-ce82-41b6-c210-4a9625071005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numerical features have been scaled. The target variable remains un-scaled.\n"
          ]
        }
      ],
      "source": [
        "# ====================================================================\n",
        "# STEP 7: Data Conversion and Feature Scaling\n",
        "# ====================================================================\n",
        "\n",
        "# Convert boolean-like columns to integers (0 or 1)\n",
        "df['E_BILLING'] = df['E_BILLING'].astype(int)\n",
        "df['EXPORT'] = df['EXPORT'].astype(int)\n",
        "\n",
        "# Define all numerical columns that should be scaled.\n",
        "# CRUCIALLY, 'NET_IMPORT_kWh' is NOT in this list.\n",
        "numerical_cols_to_scale = [\n",
        "    'TOTAL_IMPORT (kWh)', 'TOTAL_EXPORT (kWh)', 'TOTAL_IMPORT - PV1 (kWh)',\n",
        "    'TOTAL_EXPORT - PV1 (kWh)', 'TR1_TOTAL_IMPORT (kWh)', 'TR2_TOTAL_IMPORT (kWh)',\n",
        "    'TR3_TOTAL_IMPORT (kWh)', 'TR1_TOTAL_EXPORT (kWh)', 'TR2_TOTAL_EXPORT (kWh)',\n",
        "    'TR3_TOTAL_EXPORT (kWh)', 'TR1_TOTAL_IMPORT - PV1 (kWh)', 'TR2_TOTAL_IMPORT - PV1 (kWh)',\n",
        "    'TR3_TOTAL_IMPORT - PV1 (kWh)', 'TR1_TOTAL_EXPORT - PV1 (kWh)', 'TR2_TOTAL_EXPORT - PV1 (kWh)',\n",
        "    'TR3_TOTAL_EXPORT - PV1 (kWh)', 'PHASE_A_CURRENT (A)', 'PHASE_A_VOLTAGE (V)',\n",
        "    'PHASE_B_CURRENT (A)', 'PHASE_B_VOLTAGE (V)', 'PHASE_C_CURRENT (A)',\n",
        "    'PHASE_C_VOLTAGE (V)', 'FREQUENCY (Hz)', 'EXPORT_IMPORT_RATIO'\n",
        "]\n",
        "\n",
        "# Ensure we only try to scale columns that actually exist in the DataFrame\n",
        "existing_cols_to_scale = [col for col in numerical_cols_to_scale if col in df.columns]\n",
        "\n",
        "# Initialize and apply the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[existing_cols_to_scale] = scaler.fit_transform(df[existing_cols_to_scale])\n",
        "\n",
        "print(\"Numerical features have been scaled. The target variable remains un-scaled.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================================\n",
        "# STEP 9: Drop Final Irrelevant Columns\n",
        "# ====================================================================\n",
        "\n",
        "# Drop identifier columns and other columns that are not useful for a general model\n",
        "cols_to_drop = [\n",
        "    'household_ID', 'substation_ID',\n",
        "    # Mostly-missing phase B & C columns\n",
        "    'PHASE_B_CURRENT (A)', 'PHASE_B_VOLTAGE (V)',\n",
        "    'PHASE_C_CURRENT (A)', 'PHASE_C_VOLTAGE (V)'\n",
        "]\n",
        "\n",
        "# Drop only if they exist in the DataFrame\n",
        "existing_cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "df.drop(columns=existing_cols_to_drop, inplace=True)\n",
        "\n",
        "print(f\"Dropped final irrelevant columns: {existing_cols_to_drop}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9ApEiBfxxIP",
        "outputId": "46e8de12-a8bc-478b-ca33-f6e2697044e4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped final irrelevant columns: ['household_ID', 'substation_ID', 'PHASE_B_CURRENT (A)', 'PHASE_B_VOLTAGE (V)', 'PHASE_C_CURRENT (A)', 'PHASE_C_VOLTAGE (V)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef_PAPsSOEj6",
        "outputId": "a65d2955-b778-49f4-a721-f779594ea0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ FINAL CLEANED DATA SAVED â†’ /content/drive/MyDrive/WattVision_Preprocessing/data/outputs/final_cleaned_smart_6hour.csv\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# STEP 9: Save Final Cleaned Dataset\n",
        "# ==============================================================\n",
        "\n",
        "final_path = os.path.join(output_dir, 'final_cleaned_smart_6hour.csv')\n",
        "df.to_csv(final_path, index=False)\n",
        "print(f\"ðŸŽ¯ FINAL CLEANED DATA SAVED â†’ {final_path}\")"
      ]
    }
  ]
}